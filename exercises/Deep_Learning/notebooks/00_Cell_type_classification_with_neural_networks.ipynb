{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A81Q6Am1y_Ud"
   },
   "source": [
    "# Classifying cell types with neural networks\n",
    "\n",
    "In this notebook, we will build a neural network that classifies cell types in the retinal bipolar dataset for Shekhar et al., 2016. These cells have been manually annotated, and here we will show that a neural network can recapitulate these cell type labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EXFYoXoTy_Ue"
   },
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8178,
     "status": "ok",
     "timestamp": 1589305632112,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "JdWUNmk2y_Uf",
    "outputId": "c66fd4af-526e-4919-91f2-d3909f6242d0"
   },
   "outputs": [],
   "source": [
    "!pip install scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FO8aUJszy_Uk"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import scprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jxD1LoiOy_Uo"
   },
   "source": [
    "## 2. Loading the retinal bipolar data\n",
    "\n",
    "We'll use the same retinal bipolar data you saw in preprocessing and visualization.\n",
    "\n",
    "Alternatively, you may load your own data by replacing the Google Drive file ids with your own file ids.\n",
    "\n",
    "Note that if you do, you will likely not have annotated celltype labels yet. Replace all references to `metadata['CELLTYPE']` with an entry from `metadata`, or your favorite gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ndWmQm0ay_Up"
   },
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1GYqmGgv-QY6mRTJhOCE1sHWszRGMFpnf\", \"data.pickle.gz\")\n",
    "scprep.io.download.download_google_drive(\"1q1N1s044FGWzYnQEoYMDJOjdWPm_Uone\", \"metadata.pickle.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uft9tH4Hy_Ut"
   },
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\"data.pickle.gz\")\n",
    "metadata = pd.read_pickle(\"metadata.pickle.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ow4_EgqBy_Uw"
   },
   "source": [
    "#### Converting data to `numpy` format\n",
    "\n",
    "PyTorch expects data to be stored as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XD8-4J9y_Ux"
   },
   "outputs": [],
   "source": [
    "data = scprep.reduce.pca(data_raw, n_components=50, method='dense').to_numpy()\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier classification, we'll scale the data to have mean 0 and standard deviation 1 across all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data - data.mean(axis=0)\n",
    "data = data / data.std(axis=0)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, cluster_names = pd.factorize(metadata['CELLTYPE'])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60198,
     "status": "ok",
     "timestamp": 1589305695604,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "BhUTttDgy_U2",
    "outputId": "654e14cd-bd94-4e43-f9c7-f25f5534076a"
   },
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DR163lNCy_U8"
   },
   "source": [
    "#### Splitting the data into training and validation sets\n",
    "\n",
    "We'll allocate 80\\% of our data for training and 20\\% for testing. You can also do this with scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_training, data_validation, labels_training, labels_validation = train_test_split(\n",
    "    data, labels, test_size=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 59972,
     "status": "ok",
     "timestamp": 1589305695605,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "58UuUKv1y_U9",
    "outputId": "bd127f36-6498-47ae-f07b-5a2dd51aafe3"
   },
   "outputs": [],
   "source": [
    "# first let's split our data into training and validation sets\n",
    "train_test_split = int(.8 * data.shape[0])\n",
    "\n",
    "data_training = data[:train_test_split, :]\n",
    "labels_training = labels[:train_test_split]\n",
    "data_validation = data[train_test_split:, :]\n",
    "labels_validation = labels[train_test_split:]\n",
    "data_training.shape, data_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCP4K9QGy_VB"
   },
   "source": [
    "## 3. Moving Our Data to PyTorch Tensors \n",
    "\n",
    "By moving our data from numpy arrays to PyTorch Tensors, we can take advantage of the variety of tensor operations available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SmEz1AoxaHzs"
   },
   "outputs": [],
   "source": [
    "train_tensor = torch.from_numpy(data_training)\n",
    "train_labels = torch.from_numpy(labels_training)\n",
    "\n",
    "valid_tensor = torch.from_numpy(data_validation)\n",
    "valid_labels = torch.from_numpy(labels_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6TCSh3dDcZ7U"
   },
   "source": [
    "Let's go ahead and check that our tensors are the expected sizes. We can do this identically to how we've previously done it with numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58849,
     "status": "ok",
     "timestamp": 1589305695606,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "9i_UlXj_cSeN",
    "outputId": "6d6a0810-457d-40b3-f3b1-7525635ab813"
   },
   "outputs": [],
   "source": [
    "# check shapes\n",
    "print(\"train tensor shape: {}\".format(train_tensor.shape))\n",
    "print(\"train labels shape: {}\".format(train_labels.shape))\n",
    "\n",
    "print(\"valid tensor shape: {}\".format(valid_tensor.shape))\n",
    "print(\"valid labels shape: {}\".format(valid_labels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nep06YFNdh4i"
   },
   "source": [
    "## Exercise 1 - Tensor Operations 1\n",
    "\n",
    "1. Create a tensor called x of values (1,20) using torch.arange(). Check the PyTorch documentation for [help](https://pytorch.org/docs/master/generated/torch.arange.html)\n",
    "\n",
    "2. Reshape this tensor to shape (4,5)\n",
    "\n",
    "2. Add the constant 5 to x and save this tensor as y\n",
    "\n",
    "3. Power the values of y to 3rd power and save this tensor as z\n",
    "\n",
    "4. Print the first row of z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58434,
     "status": "error",
     "timestamp": 1589305695607,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "Hu3qBrTwehu_",
    "outputId": "1b846634-3f82-4fba-ff3d-e21ea20b6e6b"
   },
   "outputs": [],
   "source": [
    "# create x using torch.arange()\n",
    "x = \n",
    "\n",
    "# reshape to (4,5)\n",
    "x = \n",
    "\n",
    "# add 5\n",
    "y = \n",
    "\n",
    "# power y to the 3rd power\n",
    "z = \n",
    "\n",
    "# print the first row of z\n",
    "print("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yyq0BFieh1l"
   },
   "source": [
    "## Exercise 2 - Tensor Operations 2\n",
    "\n",
    "1. Subset the training tensor by taking the last 5 rows\n",
    "\n",
    "2. Double the values and print the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B8QoSscEc8Rc"
   },
   "outputs": [],
   "source": [
    "# Get the last five rows of `data_training`\n",
    "data_last5 = \n",
    "\n",
    "# Multiply by two\n",
    "last5_double = \n",
    "\n",
    "# Print the result\n",
    "last5_double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZnXNkmMgbQG"
   },
   "source": [
    "## 4. Building a one-layer neural network\n",
    "\n",
    "Now we know how to write simple recipes in PyTorch, we can create a more complex instruction set defining a simple neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQQbxUl3k951"
   },
   "outputs": [],
   "source": [
    "class layer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, activation=None):\n",
    "        super(layer, self).__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(input_size, output_size).double(), requires_grad=True)\n",
    "        self.bias = nn.Parameter(torch.randn(output_size).double(), requires_grad=True)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x, self.weight) + self.bias\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can make two copies of this layer and stack them together to make a simple neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1589306878346,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "5KNIbIWld4W9",
    "outputId": "f75960cf-36b2-408b-c290-c3edbe12aec4"
   },
   "outputs": [],
   "source": [
    "input_size = data_training.shape[1]\n",
    "num_hidden = 10\n",
    "\n",
    "layer_1 = layer(input_size, num_hidden, activation=nn.ReLU())\n",
    "layer_2 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "# create a hidden (middle) layer\n",
    "hidden_layer = layer_1(train_tensor)\n",
    "\n",
    "# create the output layer used to classify\n",
    "output = layer_2(hidden_layer)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UfrPOTZGhuOg"
   },
   "source": [
    "#### Build the loss function\n",
    "\n",
    "In order to train our neural network, we need to define a loss function which tells us how well (or how poorly) our classifier performed.\n",
    "\n",
    "Here, we'll use the cross-entropy loss which we discussed in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HSAT1diwiJbX"
   },
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, c_dims):\n",
    "    \"\"\"converts a N-dimensional input to a NxC dimnensional one-hot encoding\n",
    "    \"\"\"\n",
    "    y_tensor = torch.LongTensor(y_tensor)\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    c_dims = c_dims if c_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], c_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y_tensor.shape, -1)\n",
    "    return y_one_hot.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SjJxo01g-eY"
   },
   "outputs": [],
   "source": [
    "# convert our integer class labels to a binary \"one-hot\" matrix\n",
    "labels_one_hot = to_one_hot(train_labels, num_classes)\n",
    "labels_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8SjJxo01g-eY"
   },
   "outputs": [],
   "source": [
    "# compute cross entropy\n",
    "loss = labels_one_hot * torch.log(output+ 1e-6) + (1 - labels_one_hot) * torch.log(1 - output + 1e-6)\n",
    "loss = -1 * loss.sum()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a model with PyTorch\n",
    "\n",
    "Now let's use some PyTorch magic and create a model using `nn.Sequential`, which we can just treat as some fancy list for PyTorch layers. One of the benefits of this is that we can use `model.parameters()` to pull out the list of network parameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(layer_1, layer_2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now obtain the output of our model by simply calling `model(data)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(train_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Px6sHlWirB3"
   },
   "source": [
    "#### Create the optimizer\n",
    "\n",
    "To update the model parameters, PyTorch does all of the heavy lifting for us. The optimizer takes the loss value and calculates how we should change the network weights to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yF0I1uEviNuc"
   },
   "outputs": [],
   "source": [
    "# now we need an optimizer that we'll give this loss, and it'll take responsibility\n",
    "# for updating the network to make this score go down\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=learning_rate)\n",
    "\n",
    "\n",
    "# how many data points do we want to calculate at once?\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMX8M0Pl_TmR"
   },
   "source": [
    "#### Train the network\n",
    "\n",
    "Let's train the network for 100 _epochs_. An epoch is defined as having optimized our weights over all of our data points exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 147956,
     "status": "error",
     "timestamp": 1589307256803,
     "user": {
      "displayName": "Egbert Castro",
      "photoUrl": "",
      "userId": "05225301495169195138"
     },
     "user_tz": 420
    },
    "id": "wIy3I485nlJN",
    "outputId": "211e7f85-b661-4172-c845-45e7340d5879",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the network for 100 epochs\n",
    "step = 0\n",
    "print_every = 50\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(100):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEknZ39tGLqI"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "1. How did our network do? Is the classification accuracy high? \n",
    "2. How many iterations did it take for the training accuracy to stop increasing?\n",
    "3. How many iterations did it take for the training loss to stop decreasing?\n",
    "4. How many iterations did it take for the validation accuracy to stop increasing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "49RJD1tqGOaW"
   },
   "source": [
    "## Exercise 3 - network width\n",
    "\n",
    "Create a network with a wider hidden layer and compare its performance to the network with 10 hidden neurons we just built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrN_IneACxNf"
   },
   "outputs": [],
   "source": [
    "# let's set some hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "print_every = 50\n",
    "# ========\n",
    "# Build the neural network with a wider hidden layer\n",
    "num_hidden = \n",
    "# ========\n",
    "\n",
    "# build the neural network\n",
    "layer_1 = layer(input_size, num_hidden, activation=nn.ReLU())\n",
    "layer_2 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(layer_1, layer_2)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=learning_rate)\n",
    "\n",
    "# train the network\n",
    "step = 0\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(n_epochs):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrN_IneACxNf"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - optimizers\n",
    "\n",
    "PyTorch provides a number of different optimizers for us to choose from. Replace `optim.SGD` with any of `optim.RMSprop`, `optim.Adagrad` and `optim.Adam` and experiment with the number of layers and hidden units to find the best possible network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "print_every = 50\n",
    "num_hidden = 20\n",
    "\n",
    "# build the neural network\n",
    "layer_1 = layer(input_size, num_hidden, activation=nn.ReLU())\n",
    "layer_2 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "model = nn.Sequential(layer_1, layer_2)\n",
    "\n",
    "# =========\n",
    "# create the optimizer\n",
    "optimizer = optim.???(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "# =========\n",
    "\n",
    "# train the network\n",
    "step = 0\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(n_epochs):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6jgM8CMGRnQ"
   },
   "source": [
    "## Exercise 4 - network depth\n",
    "\n",
    "Create a network with *two* hidden layers and compare its performance to the network with one hidden layer we just built. Try increasing the number of epochs if the loss hasn't stopped increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFJ5vLUi_rCY"
   },
   "outputs": [],
   "source": [
    "# let's set some hyperparameters\n",
    "batch_size = 128\n",
    "# ========\n",
    "# as you add an additional layer, you may wish to decrease the learning rate\n",
    "# if the network hasn't finished improving after 100 epochs, you can train it for longer\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "# ========\n",
    "print_every = 50\n",
    "num_hidden = 20\n",
    "\n",
    "# build the neural network\n",
    "layer_1 = layer(input_size, num_hidden, activation=nn.ReLU())\n",
    "# ========\n",
    "# Create an additional hidden layer\n",
    "layer_2 = \n",
    "# ========\n",
    "layer_3 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "# create the model\n",
    "model = nn.Sequential(layer_1, layer_2, layer_3)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=learning_rate)\n",
    "\n",
    "# train the network\n",
    "step = 0\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(n_epochs):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xFJ5vLUi_rCY"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - activation functions\n",
    "\n",
    "PyTorch provides a number of different activations for us to choose from. Replace `nn.ReLU` with any of `nn.Tanh`, `nn.Sigmoid` and `nn.LeakyReLU` or any others you find in the PyTorch docs to build the best possible network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "n_epochs = 100\n",
    "print_every = 50\n",
    "num_hidden = 20\n",
    "\n",
    "# ======\n",
    "# build the neural network with your choice of activation function\n",
    "layer_1 = layer(\n",
    "    input_size, num_hidden,\n",
    "    activation=\n",
    ")\n",
    "layer_2 = layer(\n",
    "    num_hidden, num_hidden, \n",
    "    activation=\n",
    ")\n",
    "# ======\n",
    "layer_3 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "model = nn.Sequential(layer_1, layer_2, layer_3)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "# train the network\n",
    "step = 0\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(n_epochs):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "# ======"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d-akBJwCGVgJ"
   },
   "source": [
    "## Exercise 6 - more network depth\n",
    "\n",
    "Create a network with *five* hidden layers and compare its performance to the network with one hidden layer we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUMW2qppGV98"
   },
   "outputs": [],
   "source": [
    "# let's set some hyperparameters\n",
    "batch_size = 128\n",
    "# ========\n",
    "# as you add further additional layers, you may wish to decrease the learning rate, e.g. to 0.0001\n",
    "# if the network hasn't finished improving after 100 epochs, you can train it for longer\n",
    "learning_rate = \n",
    "n_epochs = \n",
    "# ========\n",
    "print_every = 50\n",
    "num_hidden = 20\n",
    "\n",
    "# ======\n",
    "# build the neural network with five hidden layers\n",
    "layer_1 = layer(\n",
    "    input_size, num_hidden,\n",
    "    activation=\n",
    ")\n",
    "layer_2 = layer(\n",
    "    num_hidden, num_hidden, \n",
    "    activation=\n",
    ")\n",
    "layer_3 = \n",
    "layer_4 = \n",
    "layer_5 =\n",
    "# ======\n",
    "\n",
    "layer_6 = layer(num_hidden, num_classes, activation=nn.Softmax(dim=-1))\n",
    "\n",
    "model = nn.Sequential(layer_1, layer_2, layer_3, layer_4, layer_5, layer_6)\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate)\n",
    "\n",
    "# train the network\n",
    "step = 0\n",
    "results = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "}\n",
    "for epoch in range(n_epochs):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(train_tensor.shape[0], train_tensor.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "      \n",
    "        train_data_batch = train_tensor[batch_indices]\n",
    "        train_labels_batch = train_labels[batch_indices]\n",
    "        train_onehot = to_one_hot(train_labels_batch, num_classes)\n",
    "\n",
    "        step += 1\n",
    "\n",
    "        # get pass batch through layers\n",
    "        output = model(train_data_batch)\n",
    "\n",
    "        # compute cross entropy\n",
    "        loss = train_onehot * torch.log(output+ 1e-6) + (1 - train_onehot) * torch.log(1 - output + 1e-6)\n",
    "        loss = -1 * loss.sum()\n",
    "\n",
    "        # backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets\n",
    "        if step % print_every == 0:\n",
    "            \n",
    "            # don't track gradients\n",
    "            with torch.no_grad():\n",
    "                # compute the predicted outputs\n",
    "                train_prediction = output.argmax(1).numpy()\n",
    "\n",
    "                # compute the accuracy over the batch\n",
    "                acc_training = np.mean(train_prediction == train_labels_batch.numpy())\n",
    "\n",
    "                # compute the loss on all the validation data\n",
    "                loss_np = []\n",
    "                output_np = []\n",
    "                labels_np = []\n",
    "\n",
    "                random_order_indices = np.random.choice(valid_tensor.shape[0], valid_tensor.shape[0], replace=False)\n",
    "\n",
    "                for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                    valid_data_batch = valid_tensor[batch_indices]\n",
    "                    valid_labels_batch = valid_labels[batch_indices]\n",
    "\n",
    "                    # pass through layers\n",
    "                    valid_output = model(valid_data_batch)\n",
    "\n",
    "                    # compute the predicted outputs\n",
    "\n",
    "                    prediction_np = valid_output.argmax(1).numpy()\n",
    "\n",
    "                    output_np.append(prediction_np.reshape(-1))\n",
    "                    labels_np.append(valid_labels_batch.numpy().reshape(-1))\n",
    "\n",
    "                    \n",
    "                # compute the accuracy over the whole dataset\n",
    "                output_np = np.concatenate(output_np)\n",
    "                labels_np = np.concatenate(labels_np)\n",
    "                acc_validation = np.mean(output_np == labels_np)\n",
    "\n",
    "                results['train_loss'].append(loss.item())\n",
    "                results['train_acc'].append(acc_training)\n",
    "                results['val_acc'].append(acc_validation)\n",
    "                print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                    step, loss.item(), acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bUMW2qppGV98"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_loss'])\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['train_acc'], label='Training accuracy')\n",
    "plt.plot(np.arange(len(results['train_loss'])) * print_every, results['val_acc'], label='Validation accuracy')\n",
    "plt.xlabel('Training step')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TQLRokIMGXkQ"
   },
   "source": [
    "### Discussion\n",
    "\n",
    "1. How did the more complex networks you built compare to the simple network we started with?\n",
    "2. Which was more useful -- adding _depth_ to your network or adding width?\n",
    "3. Which optimizer performed best?\n",
    "4. What had the biggest effect on performance: depth, width, or optimizer?\n",
    "\n",
    "#### Re-Cap\n",
    "1. Power of PyTorch is to allow us to setup the neural networks using nn.Module\n",
    "\n",
    "2. We can use the same neural network over and over with different data without having to re-write the code."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "00_Classifier_networks_draft.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/KrishnaswamyLab/SingleCellWorkshop/blob/master/exercises/Deep_Learning/notebooks/00_Classifier_networks.ipynb",
     "timestamp": 1589251939756
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
