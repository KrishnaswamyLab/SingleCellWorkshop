{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying cell types with neural networks\n",
    "\n",
    "In this notebook, we will build a neural network that classifies cell types in the retinal bipolar dataset for Shekhar et al., 2016. These cells have been manually annotated, and here we will show that a neural network can recapitulate these cell type labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user scprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import scprep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the retinal bipolar data\n",
    "\n",
    "We'll use the same retinal bipolar data you saw in preprocessing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scprep.io.download.download_google_drive(\"1pRYn62SOmmJxwVU0sSW7eBagRL2RJmx0\", \"shekhar_data.pkl\")\n",
    "scprep.io.download.download_google_drive(\"1FlNktWuJCka3pXOvNIFfRitGluZy2ftt\", \"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_pickle(\"shekhar_data.pkl\")\n",
    "clusters = pd.read_pickle(\"shekhar_clusters.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting data to `numpy` format\n",
    "\n",
    "Tensorflow expects data to be stored as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scprep.reduce.pca(data_raw, n_components=100, method='dense').to_numpy()\n",
    "labels, cluster_names = pd.factorize(clusters['CELLTYPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(labels))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and validation sets\n",
    "\n",
    "We'll allocate 80\\% of our data for training and 20\\% for testing. You can also do this with scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_training, data_validation, labels_training, labels_validation = train_test_split(\n",
    "    data, labels, test_size=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's split our data into training and validation sets\n",
    "train_test_split = int(.8 * data.shape[0])\n",
    "\n",
    "data_training = data[:train_test_split, :]\n",
    "labels_training = labels[:train_test_split]\n",
    "data_validation = data[train_test_split:, :]\n",
    "labels_validation = labels[train_test_split:]\n",
    "data_training.shape, data_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow works with an abstract computational graph. Let's create some simple operations with the first ten data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make an object in this graph corresponding to our first 10 points\n",
    "data_tf = tf.constant(data_training[:10, :], dtype=tf.float32)\n",
    "\n",
    "# and now their corresponding labels\n",
    "labels_tf = tf.constant(labels_training[:10], dtype=tf.int32)\n",
    "\n",
    "# look at the output\n",
    "print(labels_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare this to the numpy data we started with\n",
    "print(labels_training[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now go back to the original cluster names\n",
    "cluster_names[labels_training[:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `data` is a NumPy array containing actual numbers corresponding to the `cluster_names`. `data_tf`, on the other hand, is a Tensorflow variable, and is **just a set of instructions**. In this case, the instructions are extremely simple: \"take the values from this variable and make them into a constant\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow's `Session`\n",
    "\n",
    "Tensorflow variables are just **instructions** for how to do computation, *not* the actual computations themselves.\n",
    "\n",
    "In order to perform the computations as instructed, we need to start a computation session and ask it to generate the output by using `Session.run()`.\n",
    "\n",
    "For more information on computational graphs and session, see this [blog post](https://www.easy-tensorflow.com/tf-tutorials/basics/graph-and-session)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(labels_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we take the *instructions* in `data_tf` and `labels_tf` and run them with `sess.run()`, we get back the values encoded by those instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(sess.run(data_tf), data_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing mathematical recipes\n",
    "\n",
    "We can think of each instruction as a set in a recipe. When we `run` the instructions, the Tensorflow `Session` executes the recipe with the data it has been given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can now give instructions for computations on this data and then ask for the output\n",
    "w = 10 * data_tf + 3\n",
    "x = w / 2\n",
    "y = x + w\n",
    "z = y**2\n",
    "\n",
    "sess.run(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note now that the output of `sess.run(z)` is a NumPy variable that corresponds to the value of `z` after execution. We do not, however, have any NumPy arrays corresponding to `w`, `x`, or `y`.\n",
    "\n",
    "### Discussion\n",
    "1. How could we find the value of `w`, `x` or `y`?\n",
    "2. Can you think of an advantage of writing these computations as instructions, rather than storing all of the intermediate values?\n",
    "\n",
    "## Exercise 1 - Computational graphs for arithmetic\n",
    "\n",
    "Print the last 5 rows of the data matrix with their values doubled (using tensorflow operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# Get the last five rows of `data_training`\n",
    "data_last5 = \n",
    "# Create a `tf.constant` storing `data_last5`\n",
    "tf_last5 = \n",
    "# Multiply by two\n",
    "tf_last5_double =\n",
    "# Use `sess.run()` to compute the result\n",
    "data_last5_double =\n",
    "# Print the result\n",
    "data_last5_double\n",
    "# ================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building a one-layer neural network\n",
    "\n",
    "Now we know how to write simple recipes in Tensorflow, we can create a more complex instruction set defining a simple neural network with a single hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function applies the simple feedforward operation\n",
    "def layer(x, n_dim, name, activation=None):\n",
    "    # create the weight matrix\n",
    "    W = tf.get_variable(dtype=tf.float32, shape=[x.get_shape()[-1], n_dim], name='W{}'.format(name))\n",
    "    # create the bias vector\n",
    "    b = tf.get_variable(dtype=tf.float32, shape=[n_dim], name='b{}'.format(name))\n",
    "    # X2 = X1 * W + b\n",
    "    output = tf.matmul(x, W) + b\n",
    "    if activation:\n",
    "        # nonlinear activation function\n",
    "        output = activation(output)\n",
    "    return output\n",
    "\n",
    "# create a hidden (middle) layer\n",
    "hidden_layer_tf = layer(data_tf, n_dim=100, name='hidden', activation=tf.nn.relu)\n",
    "\n",
    "# create the output layer used to classify\n",
    "output_tf = layer(hidden_layer_tf, n_dim=num_classes, name='output', activation=tf.nn.softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this instruction set, `output_tf` is a Tensorflow *instruction* encoding the entire step of mathematical operations to get from the input of the neural network to the output. It does not yet contain any data!\n",
    "\n",
    "**Note Dan/Scott/Matt**: Are we going to explain `relu`, and other functions in the talks.  At least the function used the notebook(s) should be emphasized a bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train our neural network, we need to define a loss function which tells us how well (or how poorly) our classifier performed.\n",
    "\n",
    "Here, we'll use the cross-entropy loss which we discussed in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert our integer class labels to a binary \"one-hot\" matrix\n",
    "labels_one_hot = tf.one_hot(labels_tf, num_classes)\n",
    "\n",
    "# compute cross entropy\n",
    "loss_tf = labels_one_hot * tf.log(output_tf + 1e-6) + (1 - labels_one_hot) * tf.log(1 - output_tf + 1e-6)\n",
    "loss_tf = -1 * tf.reduce_sum(loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the optimizer and tell it to minimize the loss\n",
    "\n",
    "Tensorflow does all of the heavy lifting for us. The _optimizer_ takes the loss value and calculates how we should change the network weights to improve our results.  \n",
    "\n",
    "**Note Dan/Scott/Matt**: I am guessing that different optimizers will be discussed in the lectures.  If not the choice in the next code block will not make a lot of sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need an optimizer that we'll give this loss, and it'll take responsibility\n",
    "# for updating the network to make this score go down\n",
    "learning_rate = 0.00001\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# this will be the tf instruction we call for when we want to take a single step to train our network\n",
    "train_op = opt.minimize(loss_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last thing: we need to set our network weights to random values to start\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and that's it! We've built a one-layer neural network!\n",
    "\n",
    "#### Evaluating network performance\n",
    "\n",
    "Let's see how our network does at classifying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_np, labels_np = sess.run([output_tf, labels_tf])\n",
    "\n",
    "output_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we convert this output matrix into a classification? We take the column of each output with the largest value - this is the network's best guess for the label of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network outputs\n",
    "np.argmax(output_np, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do these compare to the correct answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true output labels\n",
    "labels_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look great. We can calculate this rather than having to eyeball the data each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number we got right\n",
    "print('Correct: {} / {}'.format((np.argmax(output_np, axis=1) == labels_np).sum(), output_np.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so we're not doing well yet. But here's the power of neural networks - we'll update the weights based on our performance until we start doing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the network\n",
    "\n",
    "Here's the important part: we can optimize the weights of the network based on the desired outputs and iterate until we get good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take 1000 gradient steps\n",
    "for step in range(1000):\n",
    "    # run the instruction telling tf to take one step\n",
    "    sess.run(train_op)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        # print the performance every 100 steps\n",
    "        output_np, labels_np = sess.run([output_tf, labels_tf])\n",
    "        print('Training step {} correct: {} / {}'.format(step, (np.argmax(output_np, axis=1) == labels_np).sum(), output_np.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, so our network can classify these ten points pretty well. But how can we do this for thousands or millions of points?\n",
    "\n",
    "### Start again with placeholders so we can use all of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The power of tensorflow is that we are able to define computations as we did above, but with 'placeholders' instead of actual data. We just have to define the shape and type of the variable, and then we don't have to give it actual data until we call `sess.run`.\n",
    "\n",
    "This is powerful because we can call the same computation over and over again with different data without having to rewrite the tensorflow code.\n",
    "\n",
    "So now let's start over and do it with `tf.placeholder`! Conveniently, we don't have to specify the number of rows in our dataset and can instead just use `None` to indicate this may vary from batch to batch.\n",
    "\n",
    "For more information on placeholders, check out this [tutorial](https://databricks.com/tensorflow/placeholders)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out all of the existing instructions and start again\n",
    "tf.reset_default_graph() \n",
    "\n",
    "# start a new session\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# how many data points do we want to calculate at once?\n",
    "batch_size = 10\n",
    "\n",
    "# create a placeholder which we can fill with data\n",
    "data_tf = tf.placeholder(shape=[None, data.shape[1]], dtype=tf.float32, name='data_tf')\n",
    "# and a placeholder for the corresponding labels\n",
    "labels_tf = tf.placeholder(shape=[None], dtype=tf.int32, name='labels_tf')\n",
    "\n",
    "# create the instructions to calculate the middle (hidden) layer\n",
    "hidden_layer_tf = layer(data_tf, n_dim=10, name='hidden', activation=tf.nn.relu)\n",
    "\n",
    "# create the instructions to calculate the output layer\n",
    "output_tf = layer(hidden_layer_tf, n_dim=num_classes, name='output', activation=tf.nn.softmax)\n",
    "\n",
    "# convert our numerical cluster labels to a matrix\n",
    "labels_one_hot = tf.one_hot(labels_tf, num_classes)\n",
    "\n",
    "# compute the cross entropy\n",
    "loss_tf = labels_one_hot * tf.log(output_tf + 1e-6) + (1 - labels_one_hot) * tf.log(1 - output_tf + 1e-6)\n",
    "loss_tf = - tf.reduce_sum(loss_tf)\n",
    "\n",
    "# build the optimizer\n",
    "learning_rate = 0.001\n",
    "# we'll use the AdamOptimizer as it is much more powerful\n",
    "opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "# and finally the instruction to tell tf to modify the weights to minimize the loss\n",
    "train_op = opt.minimize(loss_tf)\n",
    "\n",
    "# and finally initialize everything!\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the network\n",
    "\n",
    "Let's train the network for 100 _epochs_. An epoch is defined as having optimized our weights over all of our data points exactly once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network for 100 epochs\n",
    "step = 0\n",
    "for epoch in range(100):\n",
    "    # randomize the order in which we see the data in each epoch\n",
    "    random_order_indices = np.random.choice(data_training.shape[0], data_training.shape[0], replace=False)\n",
    "    \n",
    "    # iterate through the data in batches of size `batch_size`\n",
    "    for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "        data_batch = data_training[batch_indices]\n",
    "        labels_batch = labels_training[batch_indices]\n",
    "        step += 1\n",
    "\n",
    "        # update the weights to minimize the loss on this batch\n",
    "        _, loss_training = sess.run([train_op, loss_tf], {data_tf: data_batch, labels_tf: labels_batch})\n",
    "\n",
    "        # evaluate accuracy on both the training and validation datasets every 50 steps\n",
    "        if step % 50 == 0:\n",
    "            # compute the accuracy on the training batch\n",
    "            # compute the predicted outputs\n",
    "            output_np = sess.run(output_tf, {data_tf: data_batch})\n",
    "            # store the maximum index of each row (the prediction)\n",
    "            prediction_np = np.argmax(output_np, axis=1)\n",
    "            # compute the accuracy over the batch\n",
    "            acc_training = np.mean(prediction_np == labels_batch)\n",
    "\n",
    "            # compute the loss on all the validation data\n",
    "            loss_np = []\n",
    "            output_np = []\n",
    "            labels_np = []\n",
    "            random_order_indices = np.random.choice(data_validation.shape[0], data_validation.shape[0], replace=False)\n",
    "            for batch_indices in np.array_split(random_order_indices, random_order_indices.shape[0] // batch_size):\n",
    "                data_batch = data_validation[batch_indices]\n",
    "                labels_batch = labels_validation[batch_indices]\n",
    "                # compute the predicted outputs of each batch\n",
    "                output_np_ = sess.run(output_tf, {data_tf: data_batch})\n",
    "                # store the maximum index of each row (the prediction)\n",
    "                output_np.append(np.argmax(output_np_, axis=1))\n",
    "                # store the true labels\n",
    "                labels_np.append(labels_batch)\n",
    "\n",
    "            output_np = np.concatenate(output_np, axis=0)\n",
    "            labels_np = np.concatenate(labels_np, axis=0)\n",
    "            # compute the accuracy over the whole dataset\n",
    "            acc_validation = np.mean(output_np == labels_np)\n",
    "            print('Step {} loss: {:.3f} training accuracy: {:.3f} validation accuracy: {:.3f} '.format(\n",
    "                step, loss_training, acc_training, acc_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "How did our network do? Is the classification accuracy high? How many iterations did it take for the training accuracy to stop increasing? How many iterations did it take for the training loss to stop decreasing?\n",
    "\n",
    "#### _Breakpoint_  - once you get here, please help those around you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - network width\n",
    "\n",
    "Create a network with a wider hidden layer and compare its performance to the network with 10 hidden neurons we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ===================\n",
    "# Copy the code from above for both building the graph and training the network\n",
    "# Change n_dim in the hidden layer from 10 to something larger\n",
    "\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Breakpoint_  - once you get here, please help those around you!\n",
    "\n",
    "## Exercise 4\n",
    "\n",
    "Create a network with *two* hidden layers and compare its performance to the network with one hidden layer we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ===================\n",
    "# Copy the code from above and add another hidden layer whose input is the output of the first layer\n",
    "# The second hidden layer should be used as input to the output layer\n",
    "\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Breakpoint_  - once you get here, please help those around you!\n",
    "\n",
    "## Exercise 5\n",
    "\n",
    "Create a network with *five* hidden layers and compare its performance to the network with one hidden layer we just built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset everything\n",
    "sess.close()\n",
    "sess = tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# ===================\n",
    "# Copy the code from above and add another three hidden layers\n",
    "# Chain the output from each layer to the input at the next\n",
    "\n",
    "# ==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-Cap\n",
    "1. Power of TensorFlow is to allow us to setup the neural networks using `placeholders`.\n",
    "2. WE can use the same neural network over and over with different data without having to re-write the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
